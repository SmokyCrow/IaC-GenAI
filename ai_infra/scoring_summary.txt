AI Terraform Scoring Summary (per AI, per prompt level)
======================================================

Methodology
- Scale: 1..5 per criterion: Correctness (docker), Kubernetes fit, Storage, Image handling, Networking, Modularity, Reasoning.
- Overall score: arithmetic mean of the seven criteria.
- Basis: Scores and rationale extracted from each AI’s summary.md (P1–P3).

Claude (Sonnet 4.5)
- P1: Overall 3.1
  * Scores: Corr 3, K8s 2, Storage 3, Image 5, Net 2, Mod 2, Reason 5
  * Notes: Duplicate provider blocks; missing `/0` in Redis URLs (later fixed); monolithic main.tf; limited probes; non‑idiomatic Service ports and URLs; seven extra prompts; strong documentation artifacts.
- P2: Overall 4.1
  * Scores: Corr 4, K8s 4, Storage 5, Image 5, Net 3, Mod 4, Reason 4
  * Notes: Introduces deployment/service/pvc modules; adds probes; correct hostpath PVCs with wait_until_bound=false; still non‑idiomatic Service ports (needs explicit :8081 in URL).
- P3: Overall 4.7
  * Scores: Corr 5, K8s 5, Storage 5, Image 5, Net 5, Mod 4, Reason 4
  * Notes: Fixes dynamic blocks; adopts Service port=80 + targetPort; app composer present; solid probes and env wiring; one extra prompt; modularity at 4 per consolidation choice.
- Avg (Claude): 3.97
Claude Summary: Scores climbed as the base prompt injected more explicit structure. P1 delivered a monolith with duplicate provider blocks, missing probes, incomplete env wiring, and non‑idiomatic Service ports; seven extra prompts (providers, env matrices, args + commands, convert‑ply command, image pull policy) addressed foundational gaps and improved reasoning quality. P2 enumerated ports, PVCs, and stream/group variables; four extra prompts focused on adding commands, image pull policy, and qa‑web timeout diagnosis, lifting correctness, modularity, storage to 4–5 while networking still relied on a hardcoded port. P3 specified idiomatic Service port=80 + targetPort and an app composition module; one fix (remove dynamic blocks) completed networking; modularity scored 4 due to consolidation pattern. Progression 3.1→4.1→4.7 reflects shrinking clarification needs.

Gemini (2.5 Pro)
- P1: Overall 3.3
  * Scores: Corr 3, K8s 3, Storage 4, Image 5, Net 3, Mod 2, Reason 3
  * Notes: Schema mixups (env list vs block, ports), dependency cycle, partial probes; DNS/URL corrected; no submodules.
- P2: Overall 3.9
  * Scores: Corr 3, K8s 3, Storage 5, Image 5, Net 3, Mod 4, Reason 4
  * Notes: Adds submodules; correct PVCs with wait_until_bound=false; non‑idiomatic Service ports require explicit :8081 in URL; provider kubeconfig path added.
- P3: Overall 4.9
  * Scores: Corr 5, K8s 5, Storage 5, Image 5, Net 5, Mod 5, Reason 4
  * Notes: Fixes dynamic blocks and bash tokenization; adopts port=80 + targetPort and app composer; consistent probes; clean end‑state with ≤2 fixes total.
- Avg (Gemini): 4.03
Gemini Summary: Score growth followed increasing prompt specificity. P1’s sparse instructions triggered nine extra prompts (env matrices, stream/group corrections, schema repairs for env/ports, commands/args, volume mount attributes, image pull policy, dependency cycle removal), yielding limited correctness/modularity lift to 3.4 overall. P2 spelled out ports, PVC specs, mounts, and env groups; seven extra prompts (kubeconfig path, commands + args, syntax cleanup, image pull policy, qa‑web URL port) advanced storage to 5 and modularity/reasoning to 4 (overall 3.9). P3 provided idiomatic networking, composition, and probe guidance; two fixes (drop dynamic blocks, correct bash tokenization) completed networking and modularity at 5 with correctness rising to 4 (final 4.6). Gains derive from reduced syntax remediation and earlier structural clarity.

GPT-5
- P1: Overall 3.4
  * Scores: Corr 3, K8s 3, Storage 4, Image 5, Net 3, Mod 3, Reason 3
  * Notes: Single‑line block syntax errors; env matrices incomplete at first; PVCs added later; ingress exposure fixed.
- P2: Overall 3.7
  * Scores: Corr 3, K8s 3, Storage 4, Image 5, Net 3, Mod 4, Reason 4
  * Notes: Adds module outputs (claim_name), fixes probe blocks; resolves qa‑web timeout via explicit `:30081` in URL; still non‑idiomatic Service ports.
- P3: Overall 4.9
  * Scores: Corr 4, K8s 5, Storage 5, Image 5, Net 5, Mod 5, Reason 4
  * Notes: Idiomatic networking (Service port=80 + targetPort), exact baseline PVC sizes, and full modular composition; minor fixes resolved probe syntax and entrypoint alignment.
- Avg (GPT-5): 4.00
GPT‑5 Summary: Score changes mirror added detail in successive base prompts. P1 showed early modular separation but missed complete env matrices, PVCs, proper convert‑ply command, and full stream/group wiring; eight extra prompts (variable block syntax, env/streams/groups, DNS URLs, args + commands, convert‑ply python3, PVC introduction, ingest exposure) left correctness/storage at 3/4 (overall 3.4). P2 introduced explicit ports, PVC specs, env groups; five extra prompts (variable block syntax, PVC claim output, commands/args + probe syntax, qa‑web URL port) lifted modularity/reasoning (overall 3.7). P3’s detailed networking and composition requirements needed three fixes (remove dynamic blocks, convert single‑line probe blocks, drop incorrect ingest entrypoint) to complete networking/modularity at 5 and raise Kubernetes fit to 4; correctness and storage did not advance, capping the score at 4.3. Limited total gain stems from early modularity reducing later structural uplift while persistent correctness plateaus remained.

Averages by Prompt Level (overall score)
- P1 average: (Claude 3.1 + Gemini 3.3 + GPT‑5 3.4) / 3 = 3.27
- P2 average: (Claude 4.1 + Gemini 3.9 + GPT‑5 3.7) / 3 = 3.90
- P3 average: (Claude 4.7 + Gemini 4.9 + GPT‑5 4.9) / 3 = 4.83

Cross‑Run Observations
- Biggest gains P1→P3: Networking and Modularity (shift to Service port=80 + targetPort, addition of app composer, clearer module contracts).
- Consistently strong: Image handling (mostly 5) and Storage at higher prompts (PVC sizes/class, wait_until_bound=false, correct mounts).
- Frequent drags at lower prompts: Terraform syntax (dynamic blocks; single‑line blocks), incomplete env matrices, lack of probes, and URL port coupling.

Scoring Criteria (explanation)
- Correctness (docker): Applies and runs end‑to‑end on Docker Desktop with minimal/no fixes; penalized for schema errors or runtime breakage.
- Kubernetes fit: Idiomatic probes, labels/selectors, resource configs, and alignment with in‑cluster conventions.
- Storage: PVC presence, correct sizes/class, wait_until_bound avoidance, and correct volume mounts.
- Image handling: Proper parameterization of shared vs ingest images; Redis pin; `image_pull_policy=IfNotPresent` for local images.
- Networking: Service `port=80` with `targetPort` mapping, consistent NodePorts, and in‑cluster URLs without hardcoded container ports.
- Modularity: Use of reusable modules (`deployment`, `service`, `pvc`) and an app composition module; variables/outputs hygiene.
- Reasoning: Quality of iterative changes; avoiding regressions; solving with minimal, targeted Terraform edits.

Findings and Final Scoring Summary
----------------------------------
Aggregate Progression:
- Overall prompt-level averages rise from 3.27 (P1) → 3.90 (P2) → 4.83 (P3), a +1.56 net gain, showing strong responsiveness to added specificity.
- Largest mean criterion improvements (P1→P3 across all AIs): Networking +2.33 (2.67→5.00) and Modularity +2.33 (2.67→5.00); followed by Kubernetes fit +1.66, Storage +1.34, Correctness +1.00, Reasoning +1.00; Image handling shows 0 change (already maximal at 5).

Per‑AI Improvement Profile:
- Claude: 3.1→4.7 (+1.6). Major lifts in Networking and Storage (to 5) coincide with adoption of Service port=80 + targetPort and PVC corrections; Modularity rises with modules app/deployment/service/pvc (scored 4 in P3); Image stable at 5.
- Gemini: 3.3→4.9 (+1.6). Gains in Networking (5), Modularity (5), Correctness (5), and sustained Storage/Image (5) reflect prompt specificity and cleanup of syntax/tokenization.
- GPT‑5: 3.4→4.9 (+1.5). Strong jumps in Networking and Modularity to 5; Kubernetes fit rises to 5; Storage/Image sustained at 5; Correctness at 4 in P3.

Criterion Stability & Variability:
- Image handling is uniformly strong (all 5s at every level) and not a differentiator.
- Networking & Modularity are decisive differentiators; initial low scores reflect missing idiomatic Service wiring and lack of reusable modules/app composition.
- Correctness lags most for Claude at P1 (2) due to structural/schema errors; GPT‑5’s unchanged Correctness (3) across levels shows early adequacy but less end-state polish relative to peers.

Score Trajectory Insights:
- Sharp rises between P1 and P2 come from injecting explicit PVC specs, stream/group env matrices, and clearer NodePort + targetPort patterns; P2→P3 lifts are driven by eliminating dynamic block misuse, normalizing probe syntax, and adding app composition.
- Claude’s highest P3 (4.7) reflects lowest residual friction after modular consolidation; Gemini’s near-parity (4.6) evidences recovery from earlier structural regressions; GPT‑5’s narrower delta indicates earlier partial modularity but slower refinement of weaker criteria.

Dominant Success Factors:
- Early inclusion of: full env matrix (streams/groups), PVC mapping + sizes, Service port=80 + targetPort model, explicit command/args patterns, Redis URL with /0 suffix.
- Avoidance of: dynamic blocks for simple list attributes (command/args), single-line multi-argument probe blocks, hardcoded container ports in URLs.

Residual Weak Points at P1:
- Networking (hardcoded port coupling), Modularity (monolith main.tf), and Correctness (provider duplication, schema misuse) suppressed early scores; focused prompt enrichment systematically resolved them.

Final Ranking (Overall P3):
1. GPT‑5 4.9  2. Gemini 4.9  3. Claude 4.7

Concise Summary:
Prompt specificity chiefly amplifies Networking and Modularity scores, producing decisive overall gains. All models reach high Storage and Image handling quality early; differentiation emerges from how quickly each corrects structural wiring (ports/modules) and schema/probe syntax. High-fidelity prompts convert multi-criterion deficits into narrow residual tweaks, elevating mean overall scores into the mid–4 range.

Project File Structures (per AI, per prompt)
-------------------------------------------
Claude
- P1:
  - `semseg-terraform/`
    - `main.tf`, `variables.tf`, `outputs.tf`, `versions.tf`
- P2:
  - `terraform-k8s-semseg/`
    - `main.tf`, `variables.tf`, `outputs.tf`
    - `modules/deployment/{main.tf,variables.tf,outputs.tf}`
    - `modules/service/{main.tf,variables.tf,outputs.tf}`
    - `modules/pvc/{main.tf,variables.tf,outputs.tf}`
- P3:
  - `terraform-k8s-semseg/`
    - `main.tf`, `variables.tf`, `outputs.tf`, `terraform.tfvars.example`
    - `modules/deployment/{main.tf,variables.tf,outputs.tf}`
    - `modules/service/{main.tf,variables.tf,outputs.tf}`
    - `modules/pvc/{main.tf,variables.tf,outputs.tf}`
    - `modules/app/{main.tf,variables.tf,outputs.tf}`

Gemini
- P1:
  - `semseg-terraform/`
    - `main.tf`, `variables.tf`, `outputs.tf`, `versions.tf`
- P2:
  - `semseg_terraform/`
    - `main.tf`, `variables.tf`, `outputs.tf`, `versions.tf`
    - `modules/deployment/{main.tf,variables.tf}`
    - `modules/service/{main.tf,variables.tf}`
    - `modules/pvc/{main.tf,variables.tf}`
- P3:
  - `semseg-terraform/`
    - `main.tf`, `variables.tf`, `outputs.tf`, `versions.tf`
    - `modules/deployment/{main.tf,variables.tf}`
    - `modules/service/{main.tf,variables.tf,outputs.tf}`
    - `modules/pvc/{main.tf,variables.tf,outputs.tf}`
    - `modules/app/{main.tf,variables.tf,outputs.tf}`

GPT‑5
- P1:
  - `semseg_terraform/`
    - `versions.tf`, `providers.tf`, `variables.tf`, `main.tf`, `outputs.tf`
    - `modules/k8s-namespace/main.tf`
    - `modules/k8s-app/{main.tf,variables.tf}`
    - `modules/k8s-redis/{main.tf,variables.tf}`
- P2:
  - `semseg_terraform/`
    - `main.tf`, `variables.tf`, `outputs.tf`
    - `modules/deployment/{main.tf,variables.tf}`
    - `modules/service/{main.tf,variables.tf}`
    - `modules/pvc/{main.tf,variables.tf}`
- P3:
  - `semseg-terraform/`
    - `main.tf`, `variables.tf`, `outputs.tf`, `versions.tf`
    - `modules/deployment/{main.tf,variables.tf,outputs.tf}`
    - `modules/service/{main.tf,variables.tf,outputs.tf}`
    - `modules/pvc/{main.tf,variables.tf,outputs.tf}`
    - `modules/app/{main.tf,variables.tf,outputs.tf}`

Structure Summaries
-------------------
Overall:
- P1 structures tend to be monolithic or partially modular, with minimal outputs and occasional provider/version duplication.
- P2 introduces consistent submodules (deployment/service/pvc), clearer variables, and begins exposing helpful outputs.
- P3 standardizes module contracts and adds an `app` composition module, aligning files into `main/variables/outputs` across modules.

Per AI:
- Claude: Root‑only at P1 → core modules at P2 → adds `app` with full contracts at P3; outputs mature and example tfvars included.
- Gemini: Root‑only at P1 → core modules at P2 (partial outputs) → adds `app` and standardizes outputs at P3; versions/providers steady.
- GPT‑5: Early multi‑module (namespace/app/redis) at P1 → shared core modules at P2 → standardized `main/variables/outputs` + `app` at P3; governance files consistent.

Per Level:
- P1: Root‑centric, minimal modularity; sparse outputs; occasional provider/version duplication; governance present but contracts immature.
- P2: Core `deployment/service/pvc` modules established; clearer variable separation; outputs begin to surface claims/urls; root aggregates cleanly.
- P3: Uniform `main/variables/outputs` across modules; `app` composes and surfaces endpoints/claims; idiomatic Service wiring implied; production‑like structure.

Structure Summaries (Detailed per AI × Level)
---------------------------------------------
Claude P1:
- Monolithic `semseg-terraform` with `main/variables/outputs/versions`; no submodules; provider duplication risks; minimal outputs; early schema gaps likely (probes/env wiring) reflected in structure simplicity.
Claude P2:
- `terraform-k8s-semseg` adopts `deployment/service/pvc` modules with clean `main/variables/outputs`; root aggregates modules; outputs begin to surface claims/urls; structure supports added probes and PVC correctness.
Claude P3:
- Adds `modules/app` and example tfvars; consistent `main/variables/outputs` across modules; idiomatic composition at root; standardized Service `port=80 + targetPort` implied by module contracts; mature, production‑like layout.

Gemini P1:
- Monolithic `semseg-terraform` root with basic files; no submodules; outputs limited; early tokenization/schema mixups mirror simplistic structure; versions present for governance.
Gemini P2:
- `semseg_terraform` introduces `deployment/service/pvc` modules but some modules omit `outputs.tf`; structure shows mid‑transition maturity with providers/versions present and clearer variable separation.
Gemini P3:
- Standardized `main/variables/outputs` in all core modules plus `app`; outputs now consistent; root remains tidy; structure aligns with idiomatic networking and composition.

GPT‑5 P1:
- Starts with multiple modules (`k8s-namespace`, `k8s-app`, `k8s-redis`); root includes `providers/versions`; partial modularity but uneven outputs; early separation suggests foresight though contracts are not yet standardized.
GPT‑5 P2:
- Converges to shared `deployment/service/pvc` modules with `main/variables`; outputs appear but not yet universal; structure clearer than P1, root simpler, module boundaries better defined.
GPT‑5 P3:
- Full `main/variables/outputs` across modules plus `app`; standardized versions; root orchestrates modules; structure expresses clean contracts and surfaces URLs/claims; final layout aligns with peers’ idioms.
